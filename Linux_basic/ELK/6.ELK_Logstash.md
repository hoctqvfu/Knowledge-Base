# Tìm hiểu về Logstash trong ELK

### 1. Giới thiệu chung

- Logstash là 1 công cụ mã nguồn mở thu thập dữ liệu có khả năng liên hợp theo thời gian thực. Logstash có thể hợp nhất dữ liệu từ các nguồn khác nhau và chuẩn hóa dữ liệu ở phần xử lý tiếp theo. Loại bỏ và đồng hóa tất cả dữ liệu đó trong 1 số use case cần phân tích và thể hiện trên biểu đồ
- Logstash có 3 thành phần chính và cũng là 3 bước xử lý chính của logstash:
    - INPUT: nó có thể lấy đầu vào từ TCP/UDP, các file, từ syslog, Microsoft Windows EventLogs, STDIN và từ nhiều nguồn khác. Chúng ta có thể lấy log từ các ứng dụng trên môi trường của chúng ta rồi đẩy chúng tới Logstash
    - FILTER: khi những log này tới server logstash, có 1 số lượng lớn các bộ lọc mà cho phép ta có thể chỉnh sửa và chuyển đổi những event này. Ta có thể lấy ra các thông tin mà ta cần từ những event log
    - OUTPUT: khi xuất dữ liệu ra, logstash hỗ trợ rất nhiều các đích tới bao gồm TCP/UDP, email, các file, HTTP, Nagios và số lượng lớn các dịch vụ mạng. Ta có thể tích hợp Logstash với các công cụ tính toán số liệu (metric), các công cụ cảnh báo, các dạng biểu đồ, các công nghệ lưu trữ hay ta có thể xây dựng 1 công cụ trong môi trường làm việc của chúng ta

### 2. Cách thức hoạt động

- Luồng xử lý sự kiện của Logstash có 3 giai đoạn: Input -> Filter -> Output. Các đầu vào tạo ra các sự kiện, bộ lọc sửa đổi chúng và các đầu ra sẽ chuyển chúng tới nơi khác. Đầu vào và đầu ra hỗ trợ codec cho phép mã hóa hoặc giải mã dữ liệu khi nó vào hoặc thoát khỏi đường dẫn mà không cần phải sử dụng bộ lọc riêng biệt

- **Input:** chúng ta sử dụng input để lấy dữ liệu vào Logstash. 1 số đầu vào thường được sử dụng là:
    - File: đọc từ 1 tệp trên hệ thống, giống như lệnh ```tail -oF```
    - Syslog: nghe trên cổng 514 phổ biến cho các thông báo nhật ký hệ thống và phân tích cú pháp theo định dạng RFC 3164
    - Redis: đọc từ máy chủ Redis, sử dụng cả kênh Redis và danh sách Redis. Redis thường được sử dụng như 1 "broker" trong 1 mô hình Logstash tập trung, có hàng đợi các sự kiện Logstash từ các "shippers" từ xa
    - Beats: xử lý các sự kiện do beats gửi

- **Filter:** filter là thiết bị xử lý trung gian trong luồng của Logstash. Chúng ta có thể kết hợp các bộ lọc với các điều kiện để thực hiện 1 hành động trên 1 sự kiện nếu nó đáp ứng các tiêu chí nhất định. Một bộ lọc hữu ích bao gồm:
    - **Grok:** phân tích cú pháp và cấu trúc văn bản tùy ý - chỉnh sửa định dạng log từ client gửi về. Grok hiện là cách tốt nhất trong Logstash để phân tích cú pháp dữ liệu nhật ký không được cấu trúc thành 1 loại có cấu trúc và có thể truy vấn được. Với 120 mẫu được tích hợp sẵn trong Logstash, nhiều khả năng chúng ta sẽ tìm thấy 1 mẫu đáp ứng nhu cầu của mình
    - **Mutate:** thực hiện các phép biến đổi chung trên các trường sự kiện. Có thể đổi tên, xóa, thay thế hay sửa đổi các trường trong sự kiện của mình
    - **Drop:** xóa hoàn toàn sự kiện, VD: debug events
    - **Clone:** tạo bản sao của sự kiện, có thể thêm hoặc xóa các trường
    - **Geoip:** thêm thông tin về vị trí địa lý của địa chỉ IP (cũng hiển thị biểu đồ trong Kibana)

- **Outputs:** các đầu ra là pha cuối cùng của luồng xử lý Logstash. Một sự kiện có thể đia qua nhiều đầu ra. Một đầu ra thường được sử dụng bao gồm:
    - **Elasticsearch:** gửi dữ liệu sự kiện tới Elasticsearch. Nếu chúng ta đang có kế hoạch để lưu dữ liệu trong 1 định dạng hiệu quả, thuận tiện và dễ dàng truy vấn... Elasticsearch là 1 lựa chọn tốt
    - **File:** ghi dữ liệu sự kiện vào file trên bộ nhớ
    - **Graphite:** gửi dữ liệu sự kiện tới graphite, 1 công cụ nguồn mở phổ biến để lưu trữ và vẽ đồ thị số liệu
    - **Statsd:** gửi dữ liệu sự kiện đến statsd, 1 dịch vụ lắng nghe và thống kê

### 3. Một số mẫu khai báo Input, Filter, Output

- Input:

```sh
input {
  beats {
    port => 5044
    ssl => false
  }
}
```

- Filter:

```sh
filter {
    grok {
      match => {
        "message" => [
          "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?:? %{SSH_INVALID_USER:message}"
        ]
      }
      patterns_dir => "/etc/logstash/patterns/sshd"
      named_captures_only => true
      remove_tag => ["_grokparsefailure"]
      break_on_match => true
      add_tag => [ "SSH", "SSH_INVALID_USER" ]
      add_field => { "event_type" => "SSH_INVALID_USER" }
      overwrite => "message"
    }
}

# Grok Filter for SSH Failed Password
filter{
    grok {
      match => {
        "message" => [
          "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?:? %{SSH_FAILED_PASSWORD:message}"
        ]
      }
      patterns_dir => "/etc/logstash/patterns/sshd"
      named_captures_only => true
      remove_tag => ["_grokparsefailure"]
      break_on_match => true
      add_tag => [ "SSH", "SSH_FAILED_PASSWORD" ]
      add_field => { "event_type" => "SSH_FAILED_PASSWORD" }
      overwrite => "message"
    }
}

filter {
# Grok Filter for SSH Password Accepted

    grok {
      match => {
        "message" => [
          "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?:? %{SSH_ACCEPTED_PASSWORD}"
        ]
      }
      patterns_dir => "/etc/logstash/patterns/sshd"
      named_captures_only => true
      remove_tag => ["_grokparsefailure"]
      break_on_match => true
      add_tag => [ "SSH", "SSH_ACCEPTED_PASSWORD" ]
      add_field => { "event_type" => "SSH_ACCEPTED_PASSWORD" }
    }
}
```

- Output

```sh
output {
     elasticsearch {
       hosts => ["localhost:9200"]
       sniffing => true
       index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
     }
}
```